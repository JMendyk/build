\section{Engineering aspects}\label{sec-engineering}

In the previous sections we have modelled the most critical subset of various build systems. However, like all real-world systems, there are many corners that obscure the essence. In this section we discuss some of those details, what would need to be done to capture them in our model, and what the impact would be.

\subsection{Partial stores, exceptions}

Our model assumes a world where the store is fully-defined, every \hs{k} is associated with a \hs{v}, and every compute successfully completes returning a valid value. In the real world, build systems frequently deal with errors - e.g. ``file not found'', or ``compilation failed''. We can model such failure conditions by instantiating \hs{v} to either \hs{Maybe v} (for missing values) or \hs{Either e v} (for exceptions of type \hs{e}). That can model the the values inside the store and the \hs{compute} function, but because \hs{v} is polymorphic for builds, it does not let the build system apply special behaviour for errors - e.g. early aborting.

\subsection{Concurrency}\label{sec-concurrency}

While we have given simple implementations assuming a single thread of execution, all the build systems we address are actually concurrent. While it complicates the model to add concurrency, the complications can be restricted exclusively to the dependency strategy:

\begin{enumerate}
\item The \hs{topological} function can build the full dependency graph, and whenever all dependencies of a task are complete, the task itself can be started.
\item The \hs{reordering} function can be made parallel in a few ways, but the most direct is to have $n$ threads reading entries from the list of keys. As before, if a key requires something not yet built, it is added to the end -- the difference is that sometimes things will be moved to the back of the queue not because they are out of order but because of races with earlier nodes that had not yet finished. As a consequence, over successive runs, potentially racey dependencies will be separated, giving better parallelism over time.
\item The \hs{recursive} function can be made parallel by starting \hs{Applicative} dependencies of a \hs{Task} in parallel, using the strategy described by \citet{haxl}.
\end{enumerate}

The actual implementation of the parallel strategies is not overly onerous, but neither is it beautiful or informative.

\subsection{Impure computations}\label{sec-non-determinism}

In our model we define \hs{Task} as a function - when give the same inputs it will always produce the same outputs. Alas, the real-world is not so obliging. Some examples of impure \hs{Task} functions include:

\begin{description}
\item[Untracked state] Some actions depend on untracked state - for example your C compilation may explicitly list the \texttt{source.c} file as a dependency, but it may not record that the version of \texttt{gcc} is also a dependency.
\item[Volatility] Some actions are defined to change in every execution, for example \Excel has a function \textsf{RANDBETWEEN} which produces a random number - on each recalculation this function is defined to change. Similarly, build systems like \Make and \Shake provide \textsf{phony} rules which are also volatile.
\item[Non-determinism] Some actions are non-determinstic, producing one result from a possble set. As an example, GHC when compiled using parallelism can change the order in which unique variables are obtained from the supply, producing different but semantically identical results.
\end{description}

Interestingly, there is significant interplay between all three sources of impurity. Systems like \Bazel use various sandboxing techniques to guard against missing dependencies, but none are likely to capture all dependencies right down to CPU model and microcode version. Rules that do have untracked state can be marked as volatile, a technique \Excel takes with the \textsf{INDIRECT} function, removing the untracked-state at the cost of more recomputation.

We note that while all the simple implementations presented in \S\ref{sec-implementations} can deal with non-determinism, some real build systems cannot, e.g. \Buck.

One tempting way of modelling non-determinism is to enrich \hs{Task} from \hs{Applicative} or \hs{Monad} to \hs{Alternative} or \hs{MonadPlus} respectively - both provide the underlying class with zero and choice. However, that seems to encode a different type of non-determinism - a kind of dependency-level non-determinism when most rules really experience a value-level non-determinism.

Volatility can be modelled by introducing a special \hs{k} whose value is changed between every run.

\subsection{Cloud implementations}\label{sec-cloud-aspects}

Our model of cloud builds provides a basic framework to discuss and reason about them, but lacks a number of important engineering corners:

\begin{description}
\item[Materialisation] Sometimes input files will involve many intermediate tasks before producing the end result, e.g. an installer package. These intermediate steps may be large, so some cloud builds can execute the graph \textit{without} downloading or \textit{materialising} the results of intermediate tasks - only the final result. Some build systems can go even further, integrating with the file system to only materialise the file when the user accesses it \cite{google_internal}.
\item[Eviction] The store of traces as shown grows indefinitely, but often resource constraints require evicting old items from the store. One option is to evict the contents and any trace that mentions the now-defunct \hs{Hash}. However, if the build system can defer materialisation, it may be possible to only evict the contents, allowing builds to pass-through hashes of values where the underlying value is not known. If so, the build must be able to recreate the value if required, potentially dealing with a different result in a future run.
\item[Frankenbuilds] A build is considered a franken-build \cite{cloudbuild} if a value is calculated locally, but something that depends on that key is pulled from the cache, and the value calculated locally does not match what was previously calculated by the cloud. Our implementations avoid this issue by storing complete traces, but if a cloud build system was to only reference input nodes this situation can arise.
\item[Communication] When traces or contents are stored on a central server communication can become a bottleneck, so it is important to send only the minimum amount of information, optimising with respect to build-system specific invariants - see \S\ref{sec-smart-traces} for some possible optimisations.
\item[Offloading] Once the cloud is storing build products and traces, it is possible for the cloud to also contain dedicated workers that can execute tasks remotely - offloading some of the computation and potentially running vastly more commands in parallel.
\end{description}

\subsection{Tracking and self-tracking}\label{sec-tracking-aspects}

Some build systems, for example \Excel and \Ninja, are capable of recomputing a task if either its dependencies change, \textit{or} the rule itself changes. For example:

\begin{minted}[xleftmargin=10pt]{text}
A1 = 20            B1 = A1+A2
A2 = 10
\end{minted}

In \Excel the user can alter the value produced by \textsf{B1} by either editing the inputs of \textsf{A1} or \textsf{A2}, \textit{or} editing the formula in \textsf{B1} - e.g. to \textsf{A1-A2}. This pattern can be captured by describing the rule producing \textsf{B1} as also depending on the value \textsf{B1-formula}. In a \hs{Monad} build system the implementation can be given very directly as:

\begin{minted}{haskell}
task fetch "B1" = do
    formula <- fetch "B1-formula"
    evalFormula fetch formula
\end{minted}

Namely, first look up the formula, then interpret it. In an \hs{Applicative} system it is not possible to change dependencies based on the formula, as would be required, so instead the formula can be captured as a dependency (but its value not used) and \textit{also} baked directly into the \hs{task} function.

The build systems that have precise tracking of self-computation are all ones which use a non-Embedded Domain Specific Language to describe computations. Those which make use of a full programming language, e.g. \Shake, are much more difficult to implement equality on rules as they are arbitrary functions making use of opaque abstractions. For such build systems the incredibly pessimistic assuming of saying that any change to the build system potentially changes any rule can often be used -- the classic example being a \texttt{Makefile} depending on itself.

\subsection{Iterative computations}\label{sec-iterative-compute}

Some computations are best described not by a chain of acyclic dependencies, but by a loop - for example \LaTeX requires repeated rebuilding until it reaches a fixed point - something that can be directly expressed in build systems such as Pluto \cite{pluto}. Another example of cyclic computations is \Excel, where a cell can depend on itself, for example:

\begin{minted}{bash}
A1 = A1 + 1
\end{minted}

In such cases \Excel will normally not execute anything, but if the ``Iterative Calculations'' feature is enabled will execute the formula a maximum of $n$ times per calculation (where $n$ is a setting that defaults to 100).

For examples like \LaTeX we consider the proper encoding to not be with circular rules, but with a series of iterative steps, as described in \S\ref{neil-iterative-on-stack-overflow}. It is important that the number of executions is bounded, otherwise the build system may not terminate (a legitimate concern with \LaTeX which can be put into a situation where it is bistable or diverging over multiple executions).

The examples in \Excel tend to encode either mutable state, or recurrence relations. The former is only required because \Excel inherently lacks the ability to write mutable state, and the latter is probably better solved using explicit recurrence formulae.

Overall we choose not to deal with cyclic rules, a choice that most build systems also follow.

\subsection{Polymorphism}\label{sec-polymorphism}

Our build system abstraction assumes a \hs{k}/\hs{v} store, along with a build system that works directly on \hs{k} and \hs{v} values. However, certain build systems provide greater flexibility, e.g. \Shake permits polymorphic keys and values, allowing types that are only stored in the Shake information, and never persisted to the Store.

As one example of richer keys/values, consider the version of \textsf{gcc} - for many builds it should be a dependency. In \Shake we can define an \textit{oracle} rule as per \cite{mitchell2012shake} whose value is the result of running \hs{gcc --version} and which is volatile, making the \textsf{gcc} version something that can be depended upon. Of course, provided the build can express volatile dependencies and supports cutoff, the version number could equally be written to a file and used in a similar way.

A more compelling example is build rules that produce multiple keys -- for example \textsf{ghc Foo.hs} produces both \textsf{Foo.hi} and \textsf{Foo.o}. That can be represented by having a key whose value is a pair of file names, and whose result is a pair of file contents. From that, the rule for \textsf{Foo.hi} can be the first-component of the result of the pair. Again, such an operation can be encoded without polymorphic keys provided the pair of files (or a dummy file representing the pair) is marked as changed if either of the contained files change. Once again, polymorphic dependencies provide convenience rather than power.

Authors have remarked that polymorphism provides a much easier expression of concepts, e.g. \citet{hadrian}, but it is not essential and thus not necessary to model.
