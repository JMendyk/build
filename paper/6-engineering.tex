\section{Engineering aspects}\label{sec-engineering}

In the previous sections we have modelled the most critical subset of various
build systems. However, like all real-world systems, there are many corners that
obscure the essence. In this section we discuss some of those details, what
would need to be done to capture them in our model, and what the impact would be.

\subsection{Partial stores and exceptions}

Our model assumes a world where the store is fully-defined, every \hs{k} is
associated with a \hs{v}, and every compute successfully completes returning a
valid value. In the real world, build systems frequently deal with errors, e.g.
``file not found'', or ``compilation failed''. We can model such failure
conditions by instantiating \hs{v} to either \hs{Maybe}~\hs{v} (for missing
values) or \hs{Either}~\hs{e}~\hs{v} (for exceptions of type \hs{e}). That can
model the values inside the store and the \hs{Task}, but because \hs{v} is
polymorphic for builds, it does not let the build system apply special behaviour
for errors, e.g. early aborting.

\subsection{Parallelism}\label{sec-parallelism}

We have given simple implementations assuming a single thread of execution,
but all the build systems we address can actually build independent keys in parallel.
While it complicates the model, the complications can be restricted exclusively
to the scheduler:

\begin{enumerate}
\item The \hs{topological} scheduler can build the full dependency graph, and
whenever all dependencies of a task are complete, the task itself can be
started.

\item The \hs{restarting} scheduler can be made parallel in a few ways, but the
most direct is to have $n$ threads reading entries from the list of keys. As
before, if a key requires something not yet built, it is added to the end -- the
difference is that sometimes things will be moved to the back of the queue not
because they are out of order but because of races with earlier nodes that had
not yet finished. As a consequence, over successive runs, potentially racey
dependencies will be separated, giving better parallelism over time.

\item The \hs{suspending} function can be made parallel by starting static
dependencies of a \hs{Task} in parallel, while dynamic dependencies are being
resolved, using the strategy described by~\citet{marlow2014haxl}.
\end{enumerate}

The actual implementation of the parallel schedulers is not overly onerous,
but neither is it beautiful or informative.

\subsection{Impure computations}\label{sec-non-determinism}

In our model we define a \hs{Task} as a function -- when given the same inputs
it will always produce the same output. Alas, the real-world is not so obliging.
Some examples of impure tasks include:

\begin{description}
\item[Untracked dependencies] Some tasks depend on untracked values -- for example
C compilation will explicitly list the \cmd{source.c} file as a dependency,
but it may not record that the version of \cmd{gcc} is also a dependency.

\item[Non-determinism] Some tasks are \emph{non-deterministic}, producing a
result from a possible set. As an example, GHC when compiled using parallelism
can change the order in which unique variables are obtained from the supply,
producing different but semantically identical results.

\item[Volatility] Some tasks are defined to change in every execution, for
example \Excel provides a~``function'' \cmd{RANDBETWEEN} which produces a random
number in a specified range~--~on each recalculation it is defined to change.
Similarly, build systems like \Make and \Shake provide \emph{phony rules} which
are also volatile.
\end{description}
\simon{Is there a difference between non-determinism and volatility?  Perhaps
  one is accidental non-det while volatility is by-design non-det? But does the
  distinction matter?   Maybe for accidental non-det you are free to use an
  earlier cached result, whereas for volatity you should always recompute?
  Volatility is best handled by a dependency on RealWorld, adn we coudl say that right here.}

Interestingly, there is significant interplay between all three sources of
impurity. Systems like \Bazel use various sandboxing techniques to guard against
missing dependencies, but none are likely to capture all dependencies right down
to CPU model and microcode version. Rules that do have untracked dependencies can be
marked as volatile, a technique \Excel takes with the \cmd{INDIRECT} function,
removing the untracked dependency at the cost of minimality.

Most of the implementations in \S\ref{sec-implementations} can deal with
non-determinism, apart from \Buck, which relies on deterministic tasks, and
in turn can optimise the number of roundtrips required to the server.

One tempting way of modelling non-determinism is to enrich \hs{Tasks} from
\hs{Applicative} or \hs{Monad} to \hs{Alternative} or \hs{MonadPlus},
respectively. More concretely, the following task description corresponds to
a spreadsheet with the formula \cmd{B1 = A1 + RANDBETWEEN(1,2)}:

\vspace{1mm}
\begin{minted}[xleftmargin=10pt]{haskell}
sprsh3 :: Tasks MonadPlus String Integer
sprsh3 "B1"@\,@=@\,@Just $ Task $ \fetch -> (+) <$> fetch "A1" <*> (pure 1 <|> pure 2)
sprsh3 _   @\,@=@\,@Nothing
\end{minted}
\vspace{1mm}

\noindent
Handling such tasks is possible in our framework, but requires an adjustment of
the correctness definition (\S\ref{sec-build-correctness}): instead of requiring
that the result of recomputing the \hs{task} matches the value stored in the
\hs{result}:
\[
\hs{getValue}~\hs{k}~\hs{result}~\hs{==}~\hs{compute}~\hs{task}~\hs{result}.
\]
\noindent
we now require that \hs{result} contains \emph{one possible result of recomputing}
the \hs{task}:
\[
\hs{getValue}~\hs{k}~\hs{result}~\hs{`elem`}~\hs{computeND}~\hs{task}~\hs{result}
\]
where
\hs{computeND}~\hs{::}~\hs{Task}~\hs{MonadPlus}~\hs{k}~\hs{v}~\hs{->}~\hs{Store}~\hs{i}~\hs{k}~\hs{v}~\hs{->}~\hs{[@@v]}
returns the list of all possible results of the \hs{task} instead of just one
value (`ND' stands for `non-deterministic').

Note that \hs{Task}~\hs{MonadPlus} is powerful enough to model dependency-level
non-determinism, for example, \cmd{INDIRECT("A" \& RANDBETWEEN(1,2))}, whereas
most build tasks in real-life build systems only experience a value-level
non-determinism. \Excel handles this example simply by marking the cell
volatile~--~an approach that can be readily adopted by any of our
implementations by introducing a special key \cmd{RealWorld} whose value is
changed between every run.

\subsection{Cloud implementations}\label{sec-cloud-aspects}

Our model of cloud builds provides a basic framework to discuss and reason
about them, but lacks a number of important engineering corners:

\begin{description}
\item[Eviction] The store of traces as shown grows indefinitely, but often
resource constraints require evicting old items from the store. One option is
to evict the contents and any trace that mentions the now-defunct
\hs{Hash}~\hs{v}. However, if the build system can defer materialisation, it
may be possible to only evict the contents, allowing builds to pass-through
hashes of values where the underlying value is not known. If so, the build must
be able to recreate the value if required, potentially dealing with a different
result in a future run.

\item[Frankenbuilds] A build is considered a
\emph{frankenbuild}~\cite{esfahani2016cloudbuild} if a value is calculated
locally, but something that depends on that key is pulled from the cache, and
the value calculated locally does not match what was previously calculated and
stored in the cloud. \simon{Don't understand. The explanation in 4.2.4 is better -- and I think describes
the same thing?}
Our implementations avoid this issue by storing complete
traces, but if a cloud build system was to only reference input nodes this
situation can arise.

\item[Communication] When traces or contents are stored on a central server
communication can become a bottleneck, so it is important to send only the
minimum amount of information, optimising with respect to build-system specific
invariants.

\item[Offloading] Once the cloud is storing build products and traces, it is
possible for the cloud to also contain dedicated workers that can execute tasks
remotely~--~offloading some of the computation and potentially running vastly
more commands in parallel.

\item[Shallow builds] Sometimes input files will involve many intermediate tasks
before producing the end result, e.g. an installer package. These intermediate
steps may be large, so some cloud build systems are designed to build end
products \emph{without} downloading or \emph{materialising} the results of
intermediate tasks~--~only the final result~--~a so-called \emph{shallow build}.
Some build systems can go even further, integrating with the file system to only
materialise the file when the user accesses it \cite{gvfs}.
\end{description}

To legitimise shallow builds, we need to relax the correctness
Definition~\ref{def-correct} as follows. Let the \hs{shallow} store correspond
to the result of a shallow build. Then \hs{shallow} is correct, if \emph{there
exists} a \hs{result} which satisfies all requirements of the
Definition~\ref{def-correct}, \emph{such that} \hs{shallow} agrees with the
\hs{result} on all the input keys \hs{k}~$\in$~$I$:
\[
\hs{getValue}~\hs{k}~\hs{shallow}~\hs{==}~\hs{getValue}~\hs{k}~\hs{result}
\]
and on the target \hs{key}:
\[
\hs{getValue}~\hs{key}~\hs{shallow}~\hs{==}~\hs{getValue}~\hs{key}~\hs{result}.
\]

\noindent
This relaxes the requirements on shallow builds by dropping the constraints on
the \hs{shallow} store for all intermediate keys \hs{k}~$\in$~$O\setminus \{\hs{key}\}$.

\subsection{Tracking and self-tracking}\label{sec-tracking-aspects}

Some build systems, for example \Excel and \Ninja, are capable of recomputing a task if either its dependencies change, \emph{or} the rule itself changes. For example:

\begin{minted}[xleftmargin=10pt]{text}
A1 = 20      B1 = A1 + A2
A2 = 10
\end{minted}

\noindent
In \Excel the user can alter the value produced by \cmd{B1} by either editing
the inputs of \cmd{A1} or \cmd{A2}, \emph{or} editing the formula in
\cmd{B1}~--~e.g. to \cmd{A1 - A2}. This pattern can be captured by describing
the rule producing \cmd{B1} as also depending on the value \cmd{B1-formula}.
The implementation can be given very directly in a \hs{Tasks}~\hs{Monad} as:

\begin{minted}{haskell}
sprsh5 "B1" = Just $ Task $ \fetch -> do
    formula <- fetch "B1-formula"
    evalFormula fetch formula
\end{minted}

\noindent
Namely, first look up the formula, then interpret it. It is not possible to
change dependencies based on the formula in a \hs{Tasks}~\hs{Applicative}, as
would be required, so instead the formula can be captured as a dependency
(but its value not used) and \emph{also} baked directly into the \hs{sprsh5}
function. \simon{Don't understand.}

The build systems that have precise self-tracking are all ones which use a
\emph{non-embedded domain specific language} to describe computations.
\simon{I think you mean that you can find dependencies with executing the function,
  which seems quite difference from ``backing it into'' the Haskell code.}
Those which make use of a full programming language, e.g. \Shake, are faced with
the challenge of implementing equality on arbitrary task functions. For such
build systems the incredibly pessimistic assumption of saying that any change to
the build system potentially changes any build rule can often be used~--~the
classic example being a makefile depending on itself.

\subsection{Iterative computations}\label{sec-iterative-compute}

Some computations are best described not by a chain of acyclic dependencies,
but by a loop~--~for example \LaTeX~requires repeated rebuilding until it
reaches a fixed point~--~something that can be directly expressed in build
systems, such as \Pluto~\cite{erdweg2015pluto}. Another example of cyclic
computations is \Excel, where a cell can depend on itself, for example:

\begin{minted}[xleftmargin=10pt]{text}
A1 = A1 + 1
\end{minted}

\noindent
In such cases \Excel will normally not execute anything, but if the
``Iterative Calculations'' feature is enabled will execute the formula for a
specified maximum number $N$ of times per calculation (where $N$ is a
setting that defaults to 100).

For examples like \LaTeX~we consider the proper encoding to not be with circular
tasks, but with a series of iterative steps, as described
by~\citet{shake-fixed-point}. It is important that the number of executions is
bounded, otherwise the build system may not terminate (a legitimate concern
with \LaTeX, which can be put into a situation where it is bistable or diverging
over multiple executions).

The examples in \Excel tend to encode either mutable state, or recurrence
relations. The former is only required because \Excel inherently lacks the
ability to write mutable state, and the latter is probably better solved using
explicit recurrence formulae.

Overall we choose not to deal with cyclic rules, a choice that most build
systems also follow.

\subsection{Polymorphism}\label{sec-polymorphism}

Our build system abstraction assumes a \hs{k}/\hs{v} store, along with a build
system that works directly on \hs{k} and \hs{v} values. However, certain build
systems provide greater flexibility, e.g. \Shake permits polymorphic keys and
values, allowing types that are only stored in the \Shake information, and never
persisted to the store.

As one example of richer key/value types, consider the version of
\cmd{gcc}~--~for many builds it should be a dependency. In \Shake it is possible
to define an \emph{oracle} rule as per~\cite{mitchell2012shake} whose value is
the result of running \cmd{gcc --version} and which is volatile, making the
\cmd{gcc} version something that can be depended upon. Of course, provided the
build can express volatile dependencies and supports cutoff, the version number
could equally be written to a file and used in a similar way.

A more compelling example is build tasks that produce multiple output
keys~--~for example, \cmd{ghc Foo.hs} produces both \cmd{Foo.hi} and \cmd{Foo.o}.
That can be represented by having a key whose value is a pair of file names, and
whose result is a pair of file contents. From that, the rule for \cmd{Foo.hi}
can be the first component of the result of the pair. Again, such an operation
can be encoded without polymorphic keys provided the pair of files (or a dummy
file representing the pair) is marked as changed if either of the contained
files change. Once again, polymorphic dependencies provide convenience rather
than power.

\Shake users have remarked that polymorphism provides a much easier expression
of concepts, e.g.~\cite{hadrian}, but it is not essential and thus not necessary
to model.
