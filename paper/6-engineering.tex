\section{Engineering aspects}\label{sec-engineering}

In the previous sections we have modelled the most critical subset of various
build systems. However, like all real-world systems, there are many corners that
obscure the essence. In this section we discuss some of those details, what
would need to be done to capture them in our model, and what the impact would be.

\subsection{Partial stores and exceptions}

Our model assumes a world where the store is fully-defined, every \hs{k} is
associated with a \hs{v}, and every compute successfully completes returning a
valid value. In the real world, build systems frequently deal with errors, e.g.
``file not found'', or ``compilation failed''. We can model such failure
conditions by instantiating \hs{v} to either \hs{Maybe}~\hs{v} (for missing
values) or \hs{Either}~\hs{e}~\hs{v} (for exceptions of type \hs{e}). That can
model the values inside the store and the \hs{Task}, but because \hs{v} is
polymorphic for builds, it does not let the build system apply special behaviour
for errors, e.g. early aborting.

% Q: {Isn't ``early aborting'' automatically handled by short-circuiting
% withing monads, i.e. \hs{Nothing} or \hs{Left e} cancel out all subsequent
% computation?}
% NM says: The build system could chose an early abort monad, but the tasks are
% free in the monad, so you can't get the encoding to cross between them.

\subsection{Parallelism}\label{sec-parallelism}

While we have given simple implementations assuming a single thread of execution,
all the build systems we address can actually build independent keys in parallel.
While it complicates the model, the complications can be restricted exclusively
to the dependency strategy:

\begin{enumerate}
\item The \hs{topological} function can build the full dependency graph, and
whenever all dependencies of a task are complete, the task itself can be started.

\item The \hs{reordering} function can be made parallel in a few ways, but the
most direct is to have $n$ threads reading entries from the list of keys. As
before, if a key requires something not yet built, it is added to the end -- the
difference is that sometimes things will be moved to the back of the queue not
because they are out of order but because of races with earlier nodes that had
not yet finished. As a consequence, over successive runs, potentially racey
dependencies will be separated, giving better parallelism over time.

\item The \hs{recursive} function can be made parallel by starting static
dependencies of a \hs{Task} in parallel, while dynamic dependencies are being
resolved, using the strategy described by~\citet{marlow2014haxl}.
\end{enumerate}

The actual implementation of the parallel strategies is not overly onerous,
but neither is it beautiful or informative.

\subsection{Impure computations}\label{sec-non-determinism}

In our model we define \hs{Task} as a function -- when given the same inputs it
will always produce the same outputs. Alas, the real-world is not so obliging.
Some examples of impure tasks include:

\begin{description}
\item[Untracked state] Some tasks depend on untracked state -- for example
your C compilation may explicitly list the \cmd{source.c} file as a dependency,
but it may not record that the version of \cmd{gcc} is also a dependency.

\item[Non-determinism] Some tasks are \emph{non-deterministic}, producing a
result from a possible set. As an example, GHC when compiled using parallelism
can change the order in which unique variables are obtained from the supply,
producing different but semantically identical results.

\item[Volatility] Some tasks are defined to change in every execution, for
example \Excel provides a~``function'' \cmd{RANDBETWEEN} which produces a random
number in a specified range~--~on each recalculation it is defined to change.
Similarly, build systems like \Make and \Shake provide \emph{phony rules} which
are also volatile.
\end{description}

Interestingly, there is significant interplay between all three sources of impurity.
Systems like \Bazel use various sandboxing techniques to guard against missing
dependencies, but none are likely to capture all dependencies right down to CPU
model and microcode version. Rules that do have untracked state can be marked as
volatile, a technique \Excel takes with the \cmd{INDIRECT} function, removing the
untracked state at the cost of minimality.

Most of the implementations in \S\ref{sec-implementations} can deal with non-determinism,
apart from \Bazel and \Buck, which require deterministic execution, and in turn can
optimise the number of roundtrips required to the server.

One tempting way of modelling non-determinism is to enrich \hs{Task} from
\hs{Applicative} or \hs{Monad} to \hs{Alternative} or \hs{MonadPlus},
respectively. More concretely, the following task description corresponds to
a spreadsheet with the formula \cmd{B1 = A1 + RANDBETWEEN(1,2)}:

\vspace{1mm}
\begin{minted}[xleftmargin=10pt]{haskell}
sprsh3 :: Task MonadPlus String Integer
sprsh3 fetch "B1" = Just $ (+) <$> fetch "A1" <*> pure 1 `mplus` pure 2
sprsh3 _     _    = Nothing
\end{minted}
\vspace{1mm}

\noindent
Handling such tasks is possible in our framework, but requires an adjustment of
the correctness definition (\S\ref{sec-build-correctness}): instead of requiring
that the result of recomputing the \hs{task} matches the value stored in the \hs{result}:
\[
\hs{Just}~\hs{(}\hs{getValue}~\hs{k}~\hs{result)}~\hs{==}~\hs{compute}~\hs{task}~\hs{result}~\hs{k}\]
\noindent
we now require that \hs{result} contains \emph{one possible result of recomputing}
the \hs{task}:
\[
\hs{Just}~\hs{(}\hs{getValue}~\hs{k}~\hs{result)}~\hs{`elem`}~\hs{compute}~\hs{task}~\hs{result}~\hs{k}
\]

Note that \hs{Task}~\hs{MonadPlus} is powerful enough to model dependency-level
non-determinism, for example, \cmd{INDIRECT("A" \& RANDBETWEEN(1,2))}, whereas
most build tasks in real-life build systems only experience a value-level
non-determinism. \Excel handles this example simply by marking the cell
volatile~--~an approach that can be readily adopted by any of our
implementations by introducing a special key \cmd{RealWorld} whose value is
changed between every run.

\subsection{Cloud implementations}\label{sec-cloud-aspects}

Our model of cloud builds provides a basic framework to discuss and reason about them, but lacks a number of important engineering corners:

\begin{description}
\item[Materialisation] Sometimes input files will involve many intermediate tasks
before producing the end result, e.g. an installer package. These intermediate
steps may be large, so some cloud builds can execute the graph \emph{without}
downloading or \emph{materialising} the results of intermediate tasks -- only
the final result. Some build systems can go even further, integrating with the
file system to only materialise the file when the user accesses it \cite{gvfs}.

\item[Eviction] The store of traces as shown grows indefinitely, but often
resource constraints require evicting old items from the store. One option is
to evict the contents and any trace that mentions the now-defunct
\hs{Hash}~\hs{v}. However, if the build system can defer materialisation, it
may be possible to only evict the contents, allowing builds to pass-through
hashes of values where the underlying value is not known. If so, the build must
be able to recreate the value if required, potentially dealing with a different
result in a future run.

\item[Frankenbuilds] A build is considered a
\emph{frankenbuild}~\cite{esfahani2016cloudbuild} if a value is calculated
locally, but something that depends on that key is pulled from the cache, and
the value calculated locally does not match what was previously calculated and
stored in the cloud. Our implementations avoid this issue by storing complete
traces, but if a cloud build system was to only reference input nodes this
situation can arise.

\item[Communication] When traces or contents are stored on a central server
communication can become a bottleneck, so it is important to send only the
minimum amount of information, optimising with respect to build-system specific
invariants~--~see~\S\ref{sec-smart-traces} for some possible optimisations.

\item[Offloading] Once the cloud is storing build products and traces, it is
possible for the cloud to also contain dedicated workers that can execute tasks
remotely~--~offloading some of the computation and potentially running vastly
more commands in parallel.
\end{description}

\subsection{Tracking and self-tracking}\label{sec-tracking-aspects}

Some build systems, for example \Excel and \Ninja, are capable of recomputing a task if either its dependencies change, \emph{or} the rule itself changes. For example:

\begin{minted}[xleftmargin=10pt]{text}
A1 = 20      B1 = A1 + A2
A2 = 10
\end{minted}

\noindent
In \Excel the user can alter the value produced by \cmd{B1} by either editing
the inputs of \cmd{A1} or \cmd{A2}, \emph{or} editing the formula in
\cmd{B1}~--~e.g. to \cmd{A1 - A2}. This pattern can be captured by describing
the rule producing \cmd{B1} as also depending on the value \cmd{B1-formula}.
The implementation can be given very directly in a \hs{Task}~\hs{Monad} as:

\begin{minted}{haskell}
task fetch "B1" = do
    formula <- fetch "B1-formula"
    evalFormula fetch formula
\end{minted}

\noindent
Namely, first look up the formula, then interpret it. It is not possible to
change dependencies based on the formula in a \hs{Task}~\hs{Applicative}, as
would be required, so instead the formula can be captured as a dependency
(but its value not used) and \emph{also} baked directly into the \hs{task}
function.

The build systems that have precise self-tracking are all ones which use a
\emph{non-embedded domain specific language} to describe computations.
Those which make use of a full programming language, e.g. \Shake, are faced with
the challenge of implementing equality on arbitrary task functions. For such
build systems the incredibly pessimistic assumption of saying that any change to
the build system potentially changes any build rule can often be used~--~the
classic example being a makefile depending on itself.

\subsection{Iterative computations}\label{sec-iterative-compute}

Some computations are best described not by a chain of acyclic dependencies,
but by a loop~--~for example \LaTeX~requires repeated rebuilding until it
reaches a fixed point~--~something that can be directly expressed in build
systems, such as \Pluto~\cite{erdweg2015pluto}. Another example of cyclic
computations is \Excel, where a cell can depend on itself, for example:

\begin{minted}[xleftmargin=10pt]{text}
A1 = A1 + 1
\end{minted}

\noindent
In such cases \Excel will normally not execute anything, but if the
``Iterative Calculations'' feature is enabled will execute the formula for a
specified maximum number $N$ of times per calculation (where $N$ is a
setting that defaults to 100).

For examples like \LaTeX~we consider the proper encoding to not be with circular
tasks, but with a series of iterative steps, as described
in~\cite{shake-fixed-point}. It is important that the number of executions is
bounded, otherwise the build system may not terminate (a legitimate concern
with \LaTeX, which can be put into a situation where it is bistable or diverging
over multiple executions).

The examples in \Excel tend to encode either mutable state, or recurrence
relations. The former is only required because \Excel inherently lacks the
ability to write mutable state, and the latter is probably better solved using
explicit recurrence formulae.

Overall we choose not to deal with cyclic rules, a choice that most build
systems also follow.

\subsection{Polymorphism}\label{sec-polymorphism}

Our build system abstraction assumes a \hs{k}/\hs{v} store, along with a build
system that works directly on \hs{k} and \hs{v} values. However, certain build
systems provide greater flexibility, e.g. \Shake permits polymorphic keys and
values, allowing types that are only stored in the \Shake information, and never
persisted to the store.

As one example of richer key/value types, consider the version of
\cmd{gcc}~--~for many builds it should be a dependency. In \Shake it is possible
to define an \emph{oracle} rule as per~\cite{mitchell2012shake} whose value is
the result of running \cmd{gcc --version} and which is volatile, making the
\cmd{gcc} version something that can be depended upon. Of course, provided the
build can express volatile dependencies and supports cutoff, the version number
could equally be written to a file and used in a similar way.

A more compelling example is build tasks that produce multiple output
keys~--~for example, \cmd{ghc Foo.hs} produces both \cmd{Foo.hi} and \cmd{Foo.o}.
That can be represented by having a key whose value is a pair of file names, and
whose result is a pair of file contents. From that, the rule for \cmd{Foo.hi}
can be the first component of the result of the pair. Again, such an operation
can be encoded without polymorphic keys provided the pair of files (or a dummy
file representing the pair) is marked as changed if either of the contained
files change. Once again, polymorphic dependencies provide convenience rather
than power.

\Shake users have remarked that polymorphism provides a much easier expression
of concepts, e.g.~\cite{hadrian}, but it is not essential and thus not necessary
to model.
